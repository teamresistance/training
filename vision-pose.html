<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Pose Estimation</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="styles.css">
</head>
<body>

<!-- Header -->
<header class="site-header">
    <a href="index.html" class="site-logo">
        <div class="logo-circle">86</div>
        <div class="logo-text">
            <span class="team-name">Programming Training</span>
            <span class="team-tagline">Going against the current</span>
        </div>
    </a>
    <button class="mobile-menu-toggle" onclick="toggleMobileMenu()">☰</button>
</header>

<!-- Sidebar -->
<nav class="sidebar-nav" id="sidebarNav">
    <div class="nav-section">
        <a href="index.html">Home</a>
        <br>
        <div class="nav-section-title">Quick Reference</div>
        <ul>
            <li><a href="simulation.html">Simulation Logic</a></li>
            <li><a href="vision-pose.html">Vision Pose Estimation</a></li>
            <li><a href="vision-tracking.html">Tracking Game Pieces</a></li>
            <li><a href="vision-tuning.html">Tuning Vision</a></li>
        </ul>
    </div>
</nav>

<!-- Main Content -->
<main class="main-content">
    <div class="content-wrapper">

        <h1>Vision Pose Estimation</h1>

        <p>
            Vision pose estimation answers one question:
            <br>
            <b>“Where is the robot on the field, right now?”</b>
        </p>

        <p>
            Unlike odometry, vision provides nearly <i>absolute</i> field position. It updates significantly slower, maybe only 1 time per second if moving quickly near a tag.
            Odometry, unlike vision, updates <i>continuously</i>. However, it drifts quickly - vision corrects this.
			The goal is to fuse odometry with vision - take the less frequent but more accurate vision pose and combine it with odometry to create the most accurate pose.
        </p>

        <section>
            <h2>High-Level Data Flow</h2>

            <p>
                A complete vision pose estimate follows this pipeline:
            </p>

            <ol>
                <li>Camera detects AprilTags</li>
                <li>Camera estimates camera-to-tag transform</li>
                <li>Known tag pose converts to camera field pose</li>
                <li>Camera-to-robot transform applied</li>
                <li>Timestamp corrected for latency</li>
                <li>Estimate fused into pose estimator</li>
            </ol>

            <p>
                Every error introduced early compounds later. This is why calibration is key - if your odometry drifts slowly and your vision data isn't good you might be adding to the issue of innacuracy.
            </p>
        </section>

        <section>
            <h2>PhotonVision Pose Estimation</h2>

            <p>
                PhotonVision performs solvePnP on detected AprilTags to determine the
                3D pose of the camera relative to the tag.
            </p>

            <p>
                PhotonVision calculates the camera's pose to the to the apriltag using complex geometry - it only knows where the tag is and what it looks like.
				Using Photon's estimated pose, you can apply a transformation of the camera pose to the robot. For example, if the camera is at (1, 1) relative to
				the robot and PhotonVision estimates that that camera is at (5, 5) on the field, you can deduce that the robot is at (4, 4) on the field. (rotation is also involved)
			</p>
        </section>

        <section>
            <h2>AprilTag Field Layout</h2>

            <p>
                AprilTags become useful only because their poses are known in field
                coordinates.
            </p>

            <p>
                WPILib provides official layouts:
            </p>

            <pre><code>
AprilTagFieldLayout layout =
    AprilTagFieldLayout.loadFromResource(
        AprilTagFields.k2024Crescendo.m_resourceFile);
            </code></pre>

            <p>
                Each tag has a fixed Pose3d on the field. Vision estimates work
                backwards from that known truth.
            </p>
        </section>

        <section>
            <h2>Limelight MegaTag 1 vs MegaTag 2</h2>
			<p> It may sound easy, use the newer version! However, depending on the game and your movement, which model to use may change - it is important to test which works best.</p>

            <h3>MegaTag 1</h3>

            <p>
                MegaTag 1 estimates pose from a single tag at a time.
            </p>

            <ul>
                <li>Works at long distances</li>
                <li>Higher ambiguity</li>
                <li>More susceptible to bad angles</li>
            </ul>

            <p>
                The solvePnP problem has two valid solutions in many cases.
                PhotonVision exposes this ambiguity as a score.
            </p>

            <h3>MegaTag 2</h3>

            <p>
                MegaTag 2 can use multiple tags simultaneously to solve a constrained
                optimization problem.
            </p>

            <ul>
                <li>Much lower ambiguity</li>
                <li>Far more stable rotation</li>
                <li>Works very well with multiple tags</li>
            </ul>
        </section>

        <section>
            <h2>Camera-to-Robot Transform</h2>

            <p>
                Vision estimates the pose of the <b>camera</b>, not the robot.
                You must supply the transform from camera frame to robot frame.
            </p>

            <pre><code>
Transform3d robotToCam =
    new Transform3d(
        new Translation3d(0.25, 0.0, 0.45),
        new Rotation3d(0.0, 0.0, Math.PI));
            </code></pre>

            <p>
                If this transform is wrong:
            </p>

            <ul>
                <li>Pose estimates will shift with rotation</li>
                <li>Yaw will appear correct while X/Y drift</li>
                <li>Fusion will fight odometry, will stutter</li>
            </ul>
        </section>

        <section>
            <h2>Latency Compensation</h2>

            <p>
                Vision data is always delayed.
                This delay includes:
            </p>

            <ul>
                <li>Camera exposure time</li>
                <li>Image processing</li>
                <li>Network transport</li>
            </ul>

            <p>
                PhotonVision provides a timestamp for when the image was captured.
                This timestamp must be used.
            </p>

            <pre><code>
poseEstimator.addVisionMeasurement(
    visionPose,
    visionTimestampSeconds);
            </code></pre>

            <p>
                Without latency compensation, vision will <b>drag</b> your pose
                behind the robot during motion. Imagine always being 0.1 seconds behind! It can make a big difference.
            </p>
        </section>

        <section>
            <h2>Pose Estimator Fusion</h2>

            <p>
                WPILib uses an Extended Kalman Filter internally.
                You do not tune gains directly.
                You tune trust.
            </p>

            <p>
                Vision measurements are fused based on their covariance:
            </p>

            <ul>
                <li>Lower std dev = more trust on vision, accurate readings</li>
                <li>Higher std dev = less influence, less accurate</li>
            </ul>

            <pre><code>
poseEstimator.setVisionMeasurementStdDevs(
    VecBuilder.fill(0.7, 0.7, 1.2));
            </code></pre>

            <p>
                Poorly tuned vision std devs cause:
            </p>

            <ul>
                <li>Pose snapping</li>
                <li>Heading oscillation</li>
                <li>Auto path instability</li>
            </ul>
        </section>

        <section>
            <h2>Rejecting Bad Measurements</h2>

            <p>
                Not all vision data should be trusted. Most of these will not occur super often, but can be depending on the game and your system.
                You should reject measurements when:
            </p>

            <ul>
                <li>Ambiguity is high</li>
                <li>No apriltags (no data, obvious)</li>
                <li>Estimated Z height is invalid, Z is almost never useful</li>
            </ul>

            <p>
                Vision is a correction source, not a source of truth.
            </p>
        </section>

        <section>
            <h2>Multi-Camera Fusion</h2>

            <p>
                Multiple cameras improve robustness, not accuracy. If one camera doesn't see a tag or doesn't get a reliable reading, you can have other camera(s) fill in. If many cameras see a tag reliably, you get ultra-precise estimates.
            </p>

            <p>
                Each camera should:
            </p>

            <ul>
                <li>Have its own transform</li>
                <li>Have its own std devs</li>
                <li>Be fused with odometry independently</li>
            </ul>

            <p>
                Never average poses manually.
                Let the estimator do the math!
            </p>
        </section>

    </div>
</main>

<!-- Footer -->
<footer class="site-footer">
    <p>Team Resistance - Going against the current</p>
</footer>

<script>
    function toggleMobileMenu() {
        const sidebar = document.getElementById('sidebarNav');
        sidebar.classList.toggle('active');
    }
</script>

</body>
</html>
